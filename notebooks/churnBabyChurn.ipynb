{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter is now (2016-06-20) supported by Google Datalabs\n",
    "# Hands on example of making predictive models\n",
    "\n",
    "# Customer Churn\n",
    "\n",
    "\"Churn Rate\" - measure of the number of individuals or items moing out of a collective group over a specific period of time. - Wikipedia\n",
    "\n",
    "<img src=\"https://www.statuscake.com/wp-content/uploads/2016/02/Customer-Churn.jpg\" style=\"width:70%\" />\n",
    "\n",
    "Churn (loss of customers to competition) is a problem for any company because it is more expensive to acquire a new customer than to keep your existing one from leaving. This example we will examine the churn for telecom companies.\n",
    "\n",
    "## Binary Classification\n",
    "\n",
    "More on [classification](data-mining-intro-ML-may-2016.ipynb) in the machine learning intro notebook.\n",
    "\n",
    "\n",
    "This labeling schema is arguably the most common classification schema which is called binary classification as I have only two classes to predict(churn or not). Even some of the multi-class classification problems could be expressed as a binary classification schema(one vs other classes). \n",
    "\n",
    "<img src=\"http://www.holehouse.org/mlclass/06_Logistic_Regression_files/Image%20[23].png\" style=\"width:70%\"/>\n",
    "\n",
    "Therefore, it is a powerful/basic classification schema that you would use approximately 83%(put some random number > 50) of your machine learning problems.\n",
    "\n",
    "## Example of real application\n",
    "\n",
    "Most telecom companies suffer from voluntary churn. Churn rate has strong impact on the life time value of the customer because it affects the length of service and the future revenue of the company. \n",
    "\n",
    "* For example if a company has **25% churn rate** then the average customer lifetime is **4 years**; \n",
    "* similarly a company with a **churn rate of 50%**, has an average customer lifetime of **2 years**. \n",
    "\n",
    "It is estimated that 75 percent of the 17 to 20 million subscribers signing up with a new wireless carrier every year are coming from another wireless provider, which means they are churners. Telecom companies spend hundreds of dollars to acquire a new customer and when that customer leaves, the company not only loses the future revenue from that customer but also the resources spend to acquire that customer. Churn erodes profitability.\n",
    "\n",
    "\n",
    "Steps that have been adopted by telecom companies so far:\n",
    "\n",
    "Telecom companies have used two approaches to address churn - \n",
    "* **Untargeted approach.** The untargeted approach relies on superior product and mass advertising to increase brand loyalty and thus retain customers. \n",
    "* **Targeted approach.** The targeted approach relies on identifying customers who are likely to churn, and provide intervention to encourage them.\n",
    "\n",
    "---\n",
    "\n",
    "So what are we going to predict? We ask the business about the problem and what question there is to answer.\n",
    "\n",
    "<img src=\"../res/fig/questions.png\" style=\"width:50%\">\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data / Data Mining / Data crunching / Cleaning data\n",
    "\n",
    "<img src=\"https://icrunchdatanews.com/wp-content/uploads/2015/08/Process-getting-and-cleaning-data-v3.jpg\" >\n",
    "\n",
    "To clean data takes time. 80 % of the time is spent on cleaning the data.\n",
    "\n",
    "### Two rules of data cleaning\n",
    "\n",
    "1. Data never comes in the format you want\n",
    "2. Assuming that at least 5 % of the data is dirty\n",
    "\n",
    "---\n",
    "\n",
    "### Loading data from the CSV\n",
    "\n",
    "The data is straightforward. Each row represents a subscribing telephone customer. Each column contains customer attributes such as phone number, call minutes used during different times of day, charges incurred for services, lifetime account duration, and whether or not the customer is still a customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churn_df = pd.read_csv('../data/raw/churn.csv')\n",
    "col_names = churn_df.columns.tolist()\n",
    "\n",
    "print(\"Columns names:\",col_names)\n",
    "\n",
    "print(\"\\n Sample data:\")\n",
    "to_show = col_names[:6] + col_names[-6:]\n",
    "churn_df[to_show].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Isolate target data\n",
    "churn_result = churn_df['Churn?']\n",
    "y = np.where(churn_result == 'True.',1,0)\n",
    "\n",
    "# remove some columns, we might need them \n",
    "# but this is just for example purpose\n",
    "to_drop = ['State', 'Area Code', 'Phone', 'Churn?']\n",
    "churn_feat_space = churn_df.drop(to_drop, axis=1)\n",
    "\n",
    "# 'yes'/'no' has to be converted to boolean values\n",
    "# NumPy converts these from boolean to 1. and 0. later\n",
    "yes_no_cols = [\"Int'l Plan\",\"VMail Plan\"]\n",
    "churn_feat_space[yes_no_cols] = churn_feat_space[yes_no_cols] == 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove the test person from the training set\n",
    "churn_feat_space.drop([-2:-1])\n",
    "\n",
    "test_person = churn_feat_space[-2:-1]\n",
    "test_person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expected_test_person_churn = y[-1]\n",
    "print(expected_test_person_churn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the training data for the models\n",
    "\n",
    "Many predictors care about the relative size of different features even though those scales might be arbitrary. For instance: the number of points a basketball team scores per game will naturally be a couple orders of magnitude larger than their win percentage. But this doesn't mean that the latter is 100 times less signifigant. \n",
    "\n",
    "\n",
    "StandardScaler fixes this by normalizing each feature to a range of around 1.0 to -1.0 thereby preventing models from misbehaving. Well, at least for that reason.\n",
    "\n",
    "Great, I now have a feature space X and a set of target values y. On to the predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "churn_feat_space = churn_feat_space.fillna(0)\n",
    "X = churn_feat_space.as_matrix().astype(np.float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(\"Feature space holds %d observations and %d features\" % X.shape)\n",
    "print(\"Unique target labels:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple classifier\n",
    "**ALWAYS** start **SIMPLE.**\n",
    "\n",
    "Here we make support vector machine classifer to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clf stands for classifier and SVC stands for support vector machine\n",
    "clf = SVC(kernel='rbf')\n",
    "clf = clf.fit(X,y)\n",
    "print(\"Score\",clf.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the outcome of Ms. Test Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clf.predict(test_person))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.imgur.com/lmmBt.gif\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model the domain, but how good is your model?\n",
    "Express, test, cycle. A machine learning pipeline should be anything but static. There are always new features to design, new data to use, new classifiers to consider each with unique parameters to tune. And for every change it's critical to be able to ask, \"Is the new version better than the last?\" So how do I do that?\n",
    "\n",
    "As a good start, cross validation will be used throught this example. Cross validation attempts to avoid overfitting (training on and predicting the same datapoint) while still producing a prediction for each observation dataset. This is accomplished by systematically hiding different subsets of the data while training a set of models. After training, each model predicts on the subset that had been hidden to it, emulating multiple train-test splits. When done correctly, every observation will have a 'fair' corresponding prediction.\n",
    "\n",
    "Here's what that looks like using scikit-learn libraries.\n",
    "\n",
    "\n",
    "## Cross Validation\n",
    "Pretty much tries to fit model parameters for the data at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "def run_cv(X,y,clf_class,**kwargs):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(len(y),n_folds=3,shuffle=True)\n",
    "    y_pred = y.copy()\n",
    "    \n",
    "    # Iterate through folds\n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        # Initialize a classifier with key word arguments\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare three fairly unique algorithms support vector machines, random forest, and k-nearest-neighbors. Nothing fancy here, just passing each to cross validation and determining how often the classifier predicted the correct class.\n",
    "\n",
    "* **Support Vector Machines**\n",
    "* **Random Forest**\n",
    "* **K-nearest-neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def accuracy(y_true,y_pred):\n",
    "    # NumPy interpretes True and False as 1. and 0.\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "print(\"Accuracy\",)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"%.3f\" % accuracy(y, run_cv(X,y,LR)))\n",
    "print(\"Gradient Boosting Classifier\")\n",
    "print(\"%.3f\" % accuracy(y, run_cv(X,y,GBC)))\n",
    "print(\"Support vector machines:\")\n",
    "print(\"%.3f\" % accuracy(y, run_cv(X,y,SVC)))\n",
    "print(\"Random forest:\")\n",
    "print(\"%.3f\" % accuracy(y, run_cv(X,y,RF)))\n",
    "print(\"K-nearest-neighbors:\")\n",
    "print(\"%.3f\" % accuracy(y, run_cv(X,y,KNN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Random forest** won, right?\n",
    "\n",
    "## Precision and recall \n",
    "\n",
    "Measurements aren't golden formulas which always spit out high numbers for good models and low numbers for bad ones. Inherently they convey something sentiment about a model's performance, and it's the job of the human designer to determine each number's validity. The problem with accuracy is that outcomes aren't necessarily equal. If my classifier predicted a customer would churn and they didn't, that's not the best but it's forgivable. However, if my classifier predicted a customer would return, I didn't act, and then they churned... that's really bad.\n",
    "\n",
    "We'll be using another built in `scikit-learn` function to construction a confusion matrix. A confusion matrix is a way of visualizing predictions made by a classifier and is just a table showing the distribution of predictions for a specific class. The x-axis indicates the true class of each observation (if a customer churned or not) while the y-axis corresponds to the class predicted by the model (if my classifier said a customer would churned or not).\n",
    "\n",
    "## Confusion matrix and confusion tables: \n",
    "The columns represent the actual class and the rows represent the predicted class. Lets evaluate performance: \n",
    "\n",
    "|      | Condition True | Condition False|\n",
    "|------|----------------|---------------|\n",
    "|**Prediction True**|True Positive|False positive|\n",
    "|**Prediction False**|False Negative|True Negative|\n",
    "\n",
    "Sensitivity, Recall or True Positive Rate quantify the models ability to predict our positive classes. \n",
    "\n",
    "$$Recall = \\frac{ TP}{TP + FN}$$ \n",
    "\n",
    "Specificity or True Negative Rate quantify the models ability to predict our Negative classes. \n",
    "\n",
    "$$Precision = \\frac{ TN}{FP + TN}$$ \n",
    "\n",
    "### Example:\n",
    "\n",
    "|      | Spam | Ham|\n",
    "|------|----------------|---------------|\n",
    "|prediction Spam|100|50|\n",
    "|Prediction Ham|75|900|\n",
    "\n",
    "$$Recall = \\frac{100}{100 + 75} = 57.14 \\%   Sensitive $$\n",
    "\n",
    "$$Precision = \\frac{ 900}{50 + 900} = 94.73 \\% Specific $$\n",
    "\n",
    "Precision is useful for presenting the user with recommended items. How many selected items are relevant?\n",
    "\n",
    "Recall, how many relevant items are selected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "grad_ens_conf_matrix = metrics.confusion_matrix(y, run_cv(X, y, GBC))\n",
    "svm_svc_conf_matrix = metrics.confusion_matrix(y, run_cv(X, y, SVC))\n",
    "random_forest_conf_matrix = metrics.confusion_matrix(y, run_cv(X, y, RF))\n",
    "k_neighbors_conf_matrix = metrics.confusion_matrix(y, run_cv(X, y, KNN))\n",
    "logistic_reg_conf_matrix = metrics.confusion_matrix(y, run_cv(X, y, LR))\n",
    "dumb_conf_matrix = metrics.confusion_matrix(y, [0 for ii in y.tolist()]); # ignore the warning as they are all 0\n",
    "\n",
    "conf_matrix = {\n",
    "                1: {\n",
    "                    'matrix': grad_ens_conf_matrix,\n",
    "                    'title': 'Gradient Boosting',\n",
    "                   },\n",
    "                2: {\n",
    "                    'matrix': svm_svc_conf_matrix,\n",
    "                    'title': 'Support Vector Machine',\n",
    "                   },\n",
    "                3: {\n",
    "                    'matrix': random_forest_conf_matrix,\n",
    "                    'title': 'Random Forest',\n",
    "                   },\n",
    "                4: {\n",
    "                    'matrix': k_neighbors_conf_matrix,\n",
    "                    'title': 'K Nearest Neighbors',\n",
    "                   },\n",
    "                5: {\n",
    "                    'matrix': logistic_reg_conf_matrix,\n",
    "                    'title': 'Logistic Regression',\n",
    "                   },\n",
    "                6: {\n",
    "                    'matrix': dumb_conf_matrix,\n",
    "                    'title': 'Dumb',\n",
    "                   },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight') # Good looking plots\n",
    "fix, ax = plt.subplots(figsize=(16, 12))\n",
    "plt.suptitle('Confusion Matrix of Various Classifiers')\n",
    "for ii, values in conf_matrix.items():\n",
    "    matrix = values['matrix']\n",
    "    title = values['title']\n",
    "    plt.subplot(3, 2, ii) # starts from 1\n",
    "    plt.title(title);\n",
    "    sns.heatmap(matrix, annot=True,  fmt='');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important question to ask might be, When an individual churns, how often does my classifier predict that correctly? This measurement is called \"recall\" and a quick look at these diagrams can demonstrate that random forest is clearly best for this criteria. Out of all the churn cases (outcome \"1\") random forest correctly retrieved 330 out of 482. This translates to a churn \"recall\" of about 68% (330/482≈2/3), far better than support vector machines (≈50%) or k-nearest-neighbors (≈35%).\n",
    "\n",
    "Another question of importance is \"precision\" or, When a classifier predicts an individual will churn, how often does that individual actually churn? The differences in sematic are small from the previous question, but it makes quite a different. Random forest again out preforms the other two at about 93% precision (330 out of 356) with support vector machines a little behind at about 87% (235 out of 269). K-nearest-neighbors lags at about 80%.\n",
    "\n",
    "While, just like accuracy, precision and recall still rank random forest above SVC and KNN, this won't always be true. When different measurements do return a different pecking order, understanding the values and tradeoffs of each rating should effect how you proceed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing via a Decision Tree to explore the features!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_tree(tree, feature_names):\n",
    "    \"\"\"Create tree png using graphviz.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-learn DecsisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    \"\"\"\n",
    "    with open(\"dt.dot\", 'w') as f:\n",
    "        export_graphviz(tree, out_file=f,\n",
    "                        feature_names=feature_names)\n",
    "\n",
    "    command = [\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"]\n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "    except:\n",
    "        exit(\"Could not run dot, ie graphviz, to \"\n",
    "             \"produce visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualize_tree(clf,churn_feat_space.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Now that we have a model, we deploy it. Then we need to make a metric of how good it performs to helps us feedback to make it even better. Monitor and creating a feedback loop takes time and effort.\n",
    "\n",
    "<img src=\"http://image.slidesharecdn.com/wikujib2qoecc5r7vb7f-signature-87404360c94bb2c35661cdc792a68a5997906369b9fee3d51c5556b0e68e8418-poli-150724043821-lva1-app6891/95/production-and-beyond-deploying-and-managing-machine-learning-models-6-638.jpg?cb=1442685173\" >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
